{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coral ID\n",
    "\n",
    "Scrape one page of the Reef2Reef forums to get images of corals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlretrieve\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page from which to parse and download images\n",
    "url = 'https://www.reef2reef.com/threads/world-wide-corals-timesplitter-live-sale-3-000-frags-our-largest-ever.719326/page-15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the given url is valid\n",
    "def is_url_valid(url):\n",
    "    \"\"\"\n",
    "    Check if the format of the given url is valid.\n",
    "    \n",
    "        Parameters:\n",
    "            url (string): A string containing the url\n",
    "            \n",
    "        Returns:\n",
    "            boolean: True if url contains both scheme and netloc, else False\n",
    "    \"\"\"\n",
    "    \n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all of the images from the given url\n",
    "# currently hardcoded for posts by 'WWC-BOT' and images stored in worldwidecorals.sirv.com/TSLS_20\n",
    "def get_all_images(url):\n",
    "    \"\"\"\n",
    "    Returns all image urls from the given url.\n",
    "    \n",
    "        Parameters:\n",
    "            url (string): A string containing the url\n",
    "        \n",
    "        Returns:\n",
    "            image_links (list): A list of [names, image urls] from the page\n",
    "    \"\"\"\n",
    "    \n",
    "    soup = bs(requests.get(url).content, \"html.parser\")\n",
    "    #set([tag.name for tag in soup.find_all()])\n",
    "    \n",
    "    # extract names of each image\n",
    "    names=[]\n",
    "    temp3 = soup.find_all('div', \n",
    "                          attrs={'class':'message-userContent lbContainer js-lbContainer',\n",
    "                                 'data-lb-caption-desc':re.compile(r'^WWC-BOT')})\n",
    "    for tt in temp3:\n",
    "        t1 = tt.find_all('b')\n",
    "        for name in t1:\n",
    "            names.append(name.text)\n",
    "    names_trimmed = names[0::6]\n",
    "    \n",
    "    # extract links to each of the images\n",
    "    images=[]\n",
    "    temp3 = soup.find_all('div', \n",
    "                          attrs={'class':'message-userContent lbContainer js-lbContainer',\n",
    "                                 'data-lb-caption-desc':re.compile(r'^WWC-BOT')})\n",
    "    for tt in temp3:\n",
    "        t1 = tt.find_all('img')\n",
    "        for image in t1:\n",
    "            if 'worldwidecorals.sirv.com/TSLS_20/' in image['data-url']:\n",
    "                pos = image['data-url'].index(\"?\")\n",
    "                images.append(image['data-url'][:pos])\n",
    "    \n",
    "    # check that the lengths of the two lists are the same and if so zip together\n",
    "    if len(names_trimmed) == len(images):\n",
    "        image_links = [list(i) for i in zip(names_trimmed, images)]\n",
    "    \n",
    "    return image_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_links = get_all_images(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(image_links):\n",
    "    \"\"\"\n",
    "    Downloads all images provided in the list of image names and urls.\n",
    "    \n",
    "        Parameters:\n",
    "            image_links (list): A list of [names, image urls]\n",
    "    \"\"\"\n",
    "\n",
    "    for i in image_links:\n",
    "        filename = os.path.join('./scraped_images/', urlparse(i[1]).path.split('/')[-1])\n",
    "        urlretrieve(i[1], filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_images(image_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coral_id",
   "language": "python",
   "name": "coral_id"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
