{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coral ID - Web Scraping\n",
    "\n",
    "Live sales occur on the forums of Reef2Reef.  Pictures of frags of corals (small pieces of corals) along with their names are posted for sale.  The goal is to capture the pictures and the names to create a training set for a coral identification machine learning algorithm.\n",
    "\n",
    "The below contains code to scrape these pictures and names from one particular thread on Reef2Reef.  In the future, this can be generalized to scrape from other WWC threads on Reef2Reef and from other vendors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page from which to parse and download images\n",
    "url = 'https://www.reef2reef.com/threads/world-wide-corals-timesplitter-live-sale-3-000-frags-our-largest-ever.719326/page-15'\n",
    "# author of the posts we are pulling\n",
    "poster = 'WWC-BOT'\n",
    "# location of the stored images\n",
    "image_loc = 'worldwidecorals.sirv.com/TSLS_20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second thread for testing\n",
    "url = 'https://www.reef2reef.com/threads/world-wide-corals-tax-craze-live-sale-2300-frags-discounted-beyond-belief.552524/page-24'\n",
    "poster = 'WWC'\n",
    "image_loc = 'worldwidecorals.sirv.com/Tax_Craze_2019'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.reef2reef.com/threads/wwc-spring-lightning-sale-800-frags-up-to-75-off.712595/page-13'\n",
    "#url = 'https://www.reef2reef.com/threads/wwc-spring-lightning-sale-800-frags-up-to-75-off.712595/page-22'\n",
    "\n",
    "poster = 'WWC'\n",
    "image_loc = 'worldwidecorals.sirv.com/Spring_Lightning_Sale_2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the given url is valid\n",
    "def is_url_valid(url):\n",
    "    \"\"\"\n",
    "    Check if the format of the given url is valid.\n",
    "    Checks for scheme and netloc, then checks for status code.\n",
    "    \n",
    "        Parameters:\n",
    "            url (string): A string containing the url\n",
    "            \n",
    "        Returns:\n",
    "            is_valid (boolean): True if valid, else False\n",
    "    \"\"\"\n",
    "    \n",
    "    parsed = urlparse(url)\n",
    "    \n",
    "    is_valid = False\n",
    "    if bool(parsed.netloc) and bool(parsed.scheme):\n",
    "        if requests.get(url).status_code == 200:\n",
    "            return True\n",
    "\n",
    "    return is_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all of the images from the given url\n",
    "def get_all_images(url, poster, image_loc):\n",
    "    \"\"\"\n",
    "    Returns all image urls from the given url.\n",
    "    \n",
    "        Parameters:\n",
    "            url (string): A string containing the url\n",
    "            poster (string): A string containing the Reef2Reef author name of \n",
    "                the post, such as 'WWC-BOT'\n",
    "            image_loc (string): A string containing the location of the images,\n",
    "                such as 'worldwidecorals.sirv.com/TSLS_20' from the data-url \n",
    "                for the image\n",
    "        \n",
    "        Returns:\n",
    "            image_links (list): A list of [names, image urls] from the page\n",
    "    \"\"\"\n",
    "    \n",
    "    soup = bs(requests.get(url).content, \"html.parser\")\n",
    "    soup_div = soup.find_all('div', \n",
    "        attrs={'class':'message-userContent lbContainer js-lbContainer',\n",
    "               'data-lb-caption-desc':re.compile(r'^%s'%poster)})\n",
    "    \n",
    "    # extract links to each of the images\n",
    "    # when find an image, also extract the name for labels in training\n",
    "    images=[]\n",
    "    names=[]\n",
    "    for tt in soup_div:\n",
    "        img_found = 0\n",
    "        try:\n",
    "            t1 = tt.find_all('img')\n",
    "            for image in t1:\n",
    "                if image_loc in image['data-url']:\n",
    "                    images.append('https://www.reef2reef.com' + image['src'])\n",
    "                    img_found = 1\n",
    "            if img_found == 1:\n",
    "                t2 = tt.find_all('b')\n",
    "                for name in t2:\n",
    "                    names.append(name.text)\n",
    "        except:\n",
    "            pass\n",
    "    names_trimmed = names[0::6]\n",
    "    \n",
    "    # if the lengths of the two lists are the same, zip together\n",
    "    if len(names_trimmed) == len(images):\n",
    "        image_links = [list(i) for i in zip(names_trimmed, images)]\n",
    "    else:\n",
    "        image_links = []\n",
    "        print('WARNING: images and names are not the same length',\n",
    "              '\\n', 'no image_links this page')\n",
    "    \n",
    "    return image_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(image_links):\n",
    "    \"\"\"\n",
    "    Downloads all images provided in the list of image names and urls.\n",
    "    \n",
    "        Parameters:\n",
    "            image_links (list): A list of [names, image urls]\n",
    "    \"\"\"\n",
    "\n",
    "    for i in image_links:\n",
    "        r = requests.get(i[1])        \n",
    "        filename = os.path.join('./scraped_images/', \n",
    "                                urlparse(i[1]).query.split('%2F')[-1].split('.jpg')[0]+'.jpg')\n",
    "        with open(filename, 'wb') as outfile:\n",
    "            outfile.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add the names of the corals and the corresponding filename to a csv file for later use\n",
    "def output_names_files(image_links):\n",
    "    \"\"\"\n",
    "    Outputs a csv file with the coral name and filename.\n",
    "    \n",
    "        Parameters:\n",
    "            image_links (list): A list of [names, image urls]\n",
    "        \n",
    "        Returns:\n",
    "            coral_names_files.csv (file): \n",
    "    \"\"\"\n",
    "    \n",
    "    with open('coral_names_files.csv', 'a', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter='|')\n",
    "        writer.writerows(image_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_names_files(image_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the total number of pages in this forum thread\n",
    "def get_num_pages(url):\n",
    "    \"\"\"\n",
    "    Returns the number of pages in this thread of the forum.\n",
    "    \n",
    "        Parameters:\n",
    "            url (string): A string containing the url\n",
    "        \n",
    "        Returns:\n",
    "            num_pages (int): The number of pages\n",
    "    \"\"\"\n",
    "    \n",
    "    soup = bs(requests.get(url).content, \"html.parser\")\n",
    "    nums=[]\n",
    "    soup_ul = soup.find_all('ul', attrs={'class':'pageNav-main'})\n",
    "    for tt in soup_ul:\n",
    "        t1 = tt.find_all('a')\n",
    "        for num in t1:\n",
    "            try:\n",
    "                nums.append(int(num.text))\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    num_pages = max(nums)\n",
    "    return num_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_pages(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all of the pages and get images from the entire forum thread\n",
    "def main(url, poster, image_loc):\n",
    "    \"\"\"\n",
    "    Loops through the pages in this thread of the forum to get all images.\n",
    "    \n",
    "        Parameters:\n",
    "            url (string): A string containing the url with the starting page\n",
    "                for scraping\n",
    "            poster (string): A string containing the Reef2Reef author name\n",
    "                of the post, such as 'WWC-BOT'\n",
    "            image_loc (string): A string containing the location of the\n",
    "                images, such as 'worldwidecorals.sirv.com/TSLS_20' from the\n",
    "                data-url for the image\n",
    "    \"\"\"\n",
    "    \n",
    "    # find first page, last page, and base url\n",
    "    if is_url_valid(url):\n",
    "        last_page = get_num_pages(url)\n",
    "        base_url, first_page  = url.split('/page-')\n",
    "        first_page = int(first_page)\n",
    "        \n",
    "    # loop through pages from first to last, pausing periodically\n",
    "    print('Beginning scrape at page ', first_page)\n",
    "    for i in range(first_page, last_page+1):\n",
    "        # implement periodic pause\n",
    "        if i % 20 == 0:\n",
    "            print('pausing for 30 seconds at page ', i)\n",
    "            time.sleep(30)\n",
    "        # get url for the current page of the forum and scrape\n",
    "        current_url = base_url + '/page-' + str(i)\n",
    "        if is_url_valid(current_url):\n",
    "            image_links = get_all_images(current_url, poster, image_loc)\n",
    "            if image_links == []:\n",
    "                print('page ', i, ' has no image links')\n",
    "            download_images(image_links)\n",
    "            output_names_files(image_links)\n",
    "        else:\n",
    "            print('WARNING: page ', i, ' has invalid url')\n",
    "    print('End of scrape.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coral_id",
   "language": "python",
   "name": "coral_id"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
